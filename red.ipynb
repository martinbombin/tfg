{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Copia de tfg.ipynb","provenance":[{"file_id":"1uiL7oFxhyK5r2wMbxW8CDv4djlC8SC4C","timestamp":1624033573314},{"file_id":"1ewDkVlW5pmT6xnDNVm--t2ZZWkXXx4E1","timestamp":1612964273564}],"collapsed_sections":[],"mount_file_id":"1uiL7oFxhyK5r2wMbxW8CDv4djlC8SC4C","authorship_tag":"ABX9TyMgQZy4dniMaGHYlRrkVtyD"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vcB0zcM87baw","executionInfo":{"status":"ok","timestamp":1624030326076,"user_tz":-120,"elapsed":1375,"user":{"displayName":"Martin Barroso","photoUrl":"","userId":"04643010284075933970"}},"outputId":"d63167ab-10ad-4266-8af4-7a6c31008e68"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import keras\n","import os, sys, math\n","import tensorflow as tf\n","import pickle\n","\n","from tensorflow.keras import layers\n","from tensorflow.keras import activations\n","\n","from keras.models import Sequential\n","from keras.models import load_model\n","\n","from keras import backend as K \n","\n","from keras.layers.convolutional import Conv2D # to add convolutional layers\n","\n","from keras.layers import Conv2DTranspose\n","from keras.layers import Dense\n","from keras.layers import Flatten\n","from keras.layers import GlobalAveragePooling2D\n","from keras.layers import Reshape\n","from keras.layers import Multiply\n","\n","from scipy.io import loadmat\n","\n","device_name = tf.test.gpu_device_name()\n","#if device_name != '/device:GPU:0':\n","#  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))\n","\n","class SE_RESNet(layers.Layer): \n","  def __init__(self, strides=(1,1) , kernel_size=3, filters=128, ratio=16):   \n","    super(SE_RESNet, self).__init__() \n","\n","    self.conv1 = Conv2D(filters, kernel_size, strides, padding='same', activation='relu')\n","    self.bn1 = layers.BatchNormalization()\n","\n","    self.conv2 = Conv2D(filters, kernel_size, strides, padding='same', activation='relu')\n","    self.bn2 = layers.BatchNormalization()\n","\n","    self.aver = GlobalAveragePooling2D()\n","\n","    self.dense1 = Dense(filters // ratio, activation='relu')\n","    self.dense2 = Dense(filters, activation='sigmoid')\n","    self.reshape = Reshape([1, 1, filters])\n","\n","    self.mult = Multiply()\n","\n","\n","  def call(self, input_tensor, training=False):\n","    x = self.conv1(input_tensor)\n","    x = self.bn1(x, training=training)\n","    \n","    x = self.conv2(x)\n","    x = self.bn2(x, training=training)\n","\n","    se = self.aver(x)\n","    se = self.dense1(se)\n","    se = self.dense2(se)\n","    se = self.reshape(se)\n","    \n","    x = self.mult([x, se])\n","\n","    return x + input_tensor\n","\n","def myModel(train = 0):\n","    \n","    # create model\n","    model = Sequential()\n","\n","    layer1 = Conv2D(32, 9, strides=2, padding='same', input_shape=(24, 32, 5))\n","    layer2 = layers.BatchNormalization()\n","    if train>0:\n","      layer1.trainable = False\n","      layer2.trainable = False\n","    model.add(layer1)\n","    model.add(layers.Activation(activations.relu))\n","    model.add(layer2)\n","\n","    layer3 = Conv2D(64, 3, strides=2, padding='same')\n","    layer4 = layers.BatchNormalization()\n","    if train>1:\n","      layer3.trainable = False\n","      layer4.trainable = False\n","    model.add(layer3)\n","    model.add(layers.Activation(activations.relu))\n","    model.add(layer4)\n","\n","    layer5 = Conv2D(128, 3, padding='same')\n","    layer6 = layers.BatchNormalization()\n","    if train>2:\n","      layer5.trainable = False\n","      layer6.trainable = False\n","    model.add(layer5)\n","    model.add(layers.Activation(activations.relu))\n","    model.add(layer6)\n","\n","    layer7 = SE_RESNet()\n","    model.add(layer7)\n","\n","    layer8 = SE_RESNet()\n","    model.add(layer8)\n","\n","    layer9 = SE_RESNet()\n","    model.add(layer9)\n","\n","    layer10 = SE_RESNet()\n","    model.add(layer10)\n","\n","    layer11 = SE_RESNet()\n","    model.add(layer11)\n","\n","    layer12 = Conv2DTranspose(64, 3, strides=2, activation='relu', padding='same')\n","    layer13 = layers.BatchNormalization()\n","    if train>8:\n","      layer12.trainable = False\n","      layer13.trainable = False   \n","    model.add(layer12)\n","    model.add(layer13)  \n","\n","    layer14 = Conv2DTranspose(32, 3, strides=2, activation='relu', padding='same')\n","    layer15 = layers.BatchNormalization()\n","    if train>9:\n","      layer14.trainable = False\n","      layer15.trainable = False   \n","    model.add(layer14)\n","    model.add(layer15)\n","    \n","\n","    layer16 = Conv2D(1, 9, padding='same', activation='relu')\n","    if train>10:\n","      layer16.trainable = False\n","    model.add(layer16)\n","    \n","    # Compile model\n","    model.compile(optimizer='adam', loss='mean_squared_error')\n","\n","    return model\n","\n","# 0 -> sin normalizar, 1 -> min-max, 2 -> media y desviacion tipica\n","def initialize_data(data, type=0):\n","  #train = train[0:2000,:]\n","  train = np.copy(data)\n","  np.random.seed(0)\n","  np.random.shuffle(train)\n","\n","  trainRate = 80 #80\n","  height = 24\n","  width = 32\n","  resolution = height*width\n","\n","  felement = math.floor((train.shape[0]*trainRate)/100)\n","\n","  # min-max\n","  min = list()\n","  max = list()\n","\n","  img = list()\n","  test_img = list()\n","\n","  # A la hora de normalizar se convierte a 1 toda desviación típica que sea igual a 0\n","  for i in range(0, 6):\n","    \n","    # Muestras de entrenamiento\n","    temp_img = train[:felement,resolution*i:resolution*(i+1)]\n","\n","    # sin normalizar\n","    if (type == 0):\n","      img.append(temp_img)\n","\n","      # Muestras de test\n","      temp_img = train[felement:,resolution*i:resolution*(i+1)]\n","      #test_img.append((temp_img - mean[i]) / std[i])\n","      test_img.append(temp_img)\n","\n","    # min-max\n","    if (type == 1):\n","      if (i != 5):\n","        temp_min = np.min(temp_img, 0)\n","        temp_max = np.max(temp_img, 0)\n","        ind = temp_max==0\n","        temp_max[ind] = 1\n","        min.append(temp_min)\n","        max.append(temp_max)\n","        img.append((temp_img - min[i]) / (max[i] - min[i]))\n","\n","        # Muestras de test\n","        temp_img = train[felement:,resolution*i:resolution*(i+1)]\n","        test_img.append((temp_img - min[i]) / (max[i] - min[i]))\n","      else:\n","        img.append(temp_img)\n","\n","        # Muestras de test\n","        temp_img = train[felement:,resolution*i:resolution*(i+1)]\n","        #test_img.append((temp_img - mean[i]) / std[i])\n","        test_img.append(temp_img)\n","\n","  input = np.moveaxis(np.array([img[0], img[1], img[2], img[3], img[4]]), 0, 2) \n","  input = input.reshape(input.shape[0], 24, 32,input.shape[2])\n","  output = img[5]\n","  output = output.reshape(output.shape[0], 24, 32)\n","\n","  test_input = np.moveaxis(np.array([test_img[0], test_img[1], test_img[2], test_img[3], test_img[4]]), 0, 2)\n","  test_input = test_input.reshape(test_input.shape[0], 24, 32,test_input.shape[2])\n","  test_output = test_img[5]\n","  test_output = test_output.reshape(test_output.shape[0], 24, 32)\n","\n","  return input, output, test_input, test_output, min, max\n","\n","def normalizar_imagen(image, min, max):  \n","  # normalizamos la entrada min-max\n","  img = []\n","  for i in range(0, 5):\n","    temp_img = image[:,:,i].flatten()\n","    img.append((temp_img - min[i]) / (max[i] - min[i]))\n","\n","  input_img = np.moveaxis(np.array([img[0], img[1], img[2], img[3], img[4]]), 0, 1) \n","  input_img = input_img.reshape(24, 32,input_img.shape[1])\n","  input_img = np.array(input_img)\n","\n","  return input_img\n","\n","def dist_betw_images(x, y):\n","  return abs(np.sum(x**2 - y**2))\n","\n","if 'google.colab' in sys.modules: # Colab-only Tensorflow version selector\n","  %tensorflow_version 2.x\n","  \n","print(\"Tensorflow version \" + tf.__version__)\n","AUTO = tf.data.experimental.AUTOTUNE\n","# Detect hardware\n","try:\n","  tpu = tf.distribute.cluster_resolver.TPUClusterResolver() # TPU detection\n","except ValueError:\n","  tpu = None\n","  gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n","    \n","# Select appropriate distribution strategy for hardware\n","if tpu:\n","  tf.config.experimental_connect_to_cluster(tpu)\n","  tf.tpu.experimental.initialize_tpu_system(tpu)\n","  strategy = tf.distribute.experimental.TPUStrategy(tpu)\n","  print('Running on TPU ', tpu.master())  \n","elif len(gpus) > 0:\n","  #strategy = tf.distribute.MirroredStrategy(gpus) # this works for 1 to multiple GPUs\n","  print('Running on ', len(gpus), ' GPU(s) ')\n","  strategy=tf.distribute.OneDeviceStrategy(gpus[0])\n","else:\n","  strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n","  print('Running on CPU')\n"," \n","# How many accelerators do we have ?\n","print(\"Number of accelerators: \", strategy.num_replicas_in_sync)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found GPU at: /device:GPU:0\n","Tensorflow version 2.5.0\n","Running on  1  GPU(s) \n","Number of accelerators:  1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7omOYhSKDaej"},"source":["data = np.load(\"/content/drive/MyDrive/tfg/testDataNumpy10k.npy\")\n","tdata = data[:,:,:,:]\n","\n","np.random.seed(2)\n","np.random.shuffle(tdata)\n","\n","tdata = tdata[:,:,:,:]\n","\n","trainRate = 80 #80\n","\n","felement = math.floor((tdata.shape[0]*trainRate)/100)\n","felement = (math.floor(felement/256))*256\n","\n","train = tdata[:felement,:,:,:]\n","test = tdata[felement:,:,:,:]\n","\n","inp = train[:,:,:,:5]\n","outp = train[:,:,:,5]\n","inpt = test[:,:,:,:5]\n","outpt = test[:,:,:,5]"],"execution_count":null,"outputs":[]}]}